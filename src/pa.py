# -*- coding: utf-8 -*-
"""PA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZvV7eS48_RdVL-cLSReZubhxDMmU-aYT
"""

from google.colab import drive
drive.mount("/content/gdrive/")

"""<h1><b> Importing Libraries</b> </h1>"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix, classification_report, roc_curve, roc_auc_score, RocCurveDisplay, mean_squared_error, precision_score, precision_recall_curve, auc
from sklearn.model_selection import GridSearchCV

"""<h1><b> Load Dataset</b> </h1>"""

file_path = "/content/gdrive/My Drive/HR-Employee-Attrition.csv"

df = pd.read_csv(file_path)
df.head()

print("Shape of dataset:", df.shape)
print("Columns:", df.columns.tolist())

"""<h1><b> Data Cleaning</b> </h1>"""

df.info()

df.isnull().sum()

"""<h3>Drop irrelevant variables </h3>

"""

df.drop(["EmployeeCount","Over18","StandardHours","EmployeeNumber"], axis=1, inplace=True, errors='ignore')
df.head()

"""<h3>Outlier Detection</h3>"""

numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns

outliers = pd.DataFrame()

for col in numeric_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    outliers[col] = ((df[col] < lower) | (df[col] > upper))

outlier_counts = outliers.sum().sort_values(ascending=False)
print("Outlier counts per numeric column:\n", outlier_counts)

rows_with_outliers = df[outliers.any(axis=1)]
print(f"\nTotal rows with at least one outlier: {len(rows_with_outliers)}")

# Check duplicate rows
duplicate_rows = df[df.duplicated()]
print(f"\nNumber of duplicate rows: {duplicate_rows.shape[0]}")

"""<h1><b> EXPLORATORY DATA ANALYSIS (EDA)</b></h1>"""

sns.countplot(data=df, x="Department", hue="Attrition")
plt.title("Attrition by Department")
plt.show()

df["IncomeBand"] = pd.qcut(df["MonthlyIncome"], q=3, labels=["Low","Medium","High"])
sns.countplot(data=df, x="IncomeBand", hue="Attrition")
plt.title("Attrition by Salary Bands")
plt.show()

sns.boxplot(data=df, x="Attrition", y="YearsSinceLastPromotion")
plt.title("Attrition vs Years Since Last Promotion")
plt.show()

sns.boxplot(data=df, x="Attrition", y="YearsAtCompany")
plt.title("Attrition vs Years At Company")
plt.show()

sns.boxplot(data=df, x="Attrition", y="MonthlyIncome")
plt.title("Income vs Attrition")
plt.show()

numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns

plt.figure(figsize=(15, 6))
sns.boxplot(data=df[numeric_cols], orient='h')
plt.title("Boxplots of Numeric Columns (showing outliers)")
plt.show()

numeric_df = df.select_dtypes(include=[np.number])

plt.figure(figsize=(12,8))
sns.heatmap(numeric_df.corr(), cmap="coolwarm", annot=True, fmt=".2f", center=0)
plt.title("Correlation Heatmap (Numeric Features)")
plt.show()

"""<h1><b> Feature Engineering</b></h1>

"""

# Create CompanyExperienceRatio Feature
df["CompanyExperienceRatio"] = df["YearsAtCompany"] / df["TotalWorkingYears"].replace(0,1)

# Create PromotionGap Feature
df["PromotionGap"] = df["YearsSinceLastPromotion"] / df["YearsAtCompany"].replace(0,1)

df.head()

"""<h1><b> Data Modelling </b></h1>"""

print(df["Attrition"].unique())
print(df["Attrition"].value_counts())

df_no_outliers = df.copy()   # For Random Forest
df_logistic = df.copy()      # For Logistic Regression

for d in [df_no_outliers, df_logistic]:
    d["Attrition"] = d["Attrition"].map({"Yes": 1, "No": 0})

# Handle outliers in df_logistic only
for col in df_logistic.select_dtypes(include=['int64', 'float64']).columns:
    if col == "Attrition":
        continue
    Q1 = df_logistic[col].quantile(0.25)
    Q3 = df_logistic[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df_logistic[col] = np.where(df_logistic[col] < lower_bound, lower_bound,
                                np.where(df_logistic[col] > upper_bound, upper_bound, df_logistic[col]))

X_log = df_logistic.drop("Attrition", axis=1)
y_log = df_logistic["Attrition"]

cat_cols = X_log.select_dtypes(include=["object", "category"]).columns
X_log = pd.get_dummies(X_log, columns=cat_cols, drop_first=True)

# Train-test split
X_train_log, X_test_log, y_train_log, y_test_log = train_test_split(
    X_log, y_log, test_size=0.2, random_state=42, stratify=y_log
)

# Feature scaling
scaler = StandardScaler()
X_train_log = scaler.fit_transform(X_train_log)
X_test_log = scaler.transform(X_test_log)

"""<h3>Fit Logistic Regression Model</h3>"""

log_reg_model = LogisticRegression(max_iter=1000, random_state=42,verbose=1)
log_reg_model.fit(X_train_log, y_train_log)

"""<h3>Predict Logistic Regression Model</h3>"""

y_pred = log_reg_model.predict(X_test_log)
y_proba = log_reg_model.predict_proba(X_test_log)[:,1]

"""<h3>Evaluation</h3>"""

acc = accuracy_score(y_test_log, y_pred)
print(f"Logistic Regression Accuracy: {acc:.4f}\n")
print("Classification Report:\n", classification_report(y_test_log, y_pred))

"""<h3>Confusion Matrix Heatmap</h3>"""

plt.figure(figsize=(5, 4))
sns.heatmap(confusion_matrix(y_test_log, y_pred), annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix - Logistic Regression")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""<h3>ROC Curve</h3>"""

fpr_log, tpr_log, _ = roc_curve(y_test_log, y_proba)
auc_log = roc_auc_score(y_test_log, y_proba)
print(f"Logistic Regression AUC: {auc_log:.4f}")

roc_df_log = pd.DataFrame({'FPR': fpr_log, 'TPR': tpr_log})

plt.figure(figsize=(6,5))
sns.lineplot(data=roc_df_log, x='FPR', y='TPR', label=f'Logistic Regression (AUC={auc_log:.3f})')
plt.plot([0,1], [0,1], 'k--', label='Random Guess')
plt.title("ROC Curve - Logistic Regression")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.show()

"""<h3>Root Mean Square Error</h3>"""

rmse_log = np.sqrt(mean_squared_error(y_test_log, y_proba))
print(f"Logistic Regression RMSE: {rmse_log:.4f}")

"""<h3>Fit Random Forest Classifier Model</h3>"""

X_rf = df_no_outliers.drop("Attrition", axis=1)
y_rf = df_no_outliers["Attrition"]

cat_cols = X_rf.select_dtypes(include=["object", "category"]).columns
X_rf = pd.get_dummies(X_rf, columns=cat_cols, drop_first=True)

X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(
    X_rf, y_rf, test_size=0.2, random_state=42, stratify=y_rf
)

rf = RandomForestClassifier(
    n_estimators=200,
    random_state=42,
    class_weight='balanced',
    max_depth=None
)
rf.fit(X_train_rf, y_train_rf)

"""<h3>Predict Random Forest Classifier Model</h3>"""

y_pred_rf = rf.predict(X_test_rf)
y_prob_rf = rf.predict_proba(X_test_rf)[:, 1]

"""<h3>Evaluation</h3>"""

acc_rf = accuracy_score(y_test_rf, y_pred_rf)
print(f"Random Forest Accuracy: {acc_rf:.4f}\n")
print("Classification Report:\n", classification_report(y_test_rf, y_pred_rf))

"""<h3>Confusion Matrix Heatmap</h3>"""

cm = confusion_matrix(y_test_rf, y_pred_rf)
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title("Confusion Matrix - Random Forest")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

"""<h3>ROC Curve</h3>"""

fpr_rf, tpr_rf, _ = roc_curve(y_test_rf, y_prob_rf)
auc_rf = roc_auc_score(y_test_rf, y_prob_rf)
roc_df = pd.DataFrame({'FPR': fpr_rf, 'TPR': tpr_rf})
print(f"Random Forest AUC: {auc_rf:.4f}")

plt.figure(figsize=(6,5))
sns.lineplot(data=roc_df, x='FPR', y='TPR', label=f"Random Forest (AUC = {auc_rf:.4f})")
plt.plot([0,1], [0,1], 'k--', label='Random Guess')
plt.title("ROC Curve - Random Forest")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.show()

"""<h3>Root Mean Square Error</h3>"""

rmse_rf = np.sqrt(mean_squared_error(y_test_rf, y_prob_rf))
print(f"Random Forest RMSE: {rmse_rf:.4f}")

"""<h3>Feature Importance</h3>"""

if hasattr(X_rf, "columns"):
    feat_names = X_rf.columns
else:
    feat_names = [f"f{i}" for i in range(X_train.shape[1])]

importances = rf.feature_importances_
feat_imp = pd.Series(importances, index=feat_names).sort_values(ascending=False).head(20)

plt.figure(figsize=(8,6))
sns.barplot(x=feat_imp.values, y=feat_imp.index, palette='viridis')
plt.title("Top 20 Feature Importances - Random Forest")
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

"""<h1>COMPARISON</h1>"""

acc_log = accuracy_score(y_test_log, y_pred)
f1_log = f1_score(y_test_log, y_pred)
prec_log = precision_score(y_test_log, y_pred)
rec_log = recall_score(y_test_log, y_pred)

acc_rf = accuracy_score(y_test_rf, y_pred_rf)
f1_rf = f1_score(y_test_rf, y_pred_rf)
prec_rf = precision_score(y_test_rf, y_pred_rf)
rec_rf = recall_score(y_test_rf, y_pred_rf)

print(f"LogReg - Acc:{acc_log:.3f}, F1:{f1_log:.3f}, Prec:{prec_log:.3f}, Rec:{rec_log:.3f}")
print(f"RandomForest - Acc:{acc_rf:.3f}, F1:{f1_rf:.3f}, Prec:{prec_rf:.3f}, Rec:{rec_rf:.3f}")

prec_log, rec_log, _ = precision_recall_curve(y_test_log, y_proba)
auc_pr_log = auc(rec_log, prec_log)

prec_rf, rec_rf, _ = precision_recall_curve(y_test_rf, y_prob_rf)
auc_pr_rf = auc(rec_rf, prec_rf)

plt.figure(figsize=(6,5))
sns.lineplot(x=rec_log, y=prec_log, label=f'LogReg (AUC_PR={auc_pr_log:.3f})')
sns.lineplot(x=rec_rf, y=prec_rf, label=f'RandomForest (AUC_PR={auc_pr_rf:.3f})')
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.legend()
plt.show()

auc_log = roc_auc_score(y_test_log, y_proba)
print(f"Logistic Regression AUC: {auc_log:.4f}")
print(f"Random Forest AUC: {auc_rf:.4f}")

roc_df_log = pd.DataFrame({'FPR': fpr_log, 'TPR': tpr_log, 'Model': 'Logistic Regression'})
roc_df_rf = pd.DataFrame({'FPR': fpr_rf, 'TPR': tpr_rf, 'Model': 'Random Forest'})
roc_df = pd.concat([roc_df_log, roc_df_rf])

plt.figure(figsize=(7,6))
sns.lineplot(data=roc_df, x='FPR', y='TPR', hue='Model')
plt.plot([0,1], [0,1], 'k--', label='Random Guess')
plt.title("ROC Curve Comparison: Logistic Regression vs Random Forest")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.show()

print(f"Logistic Regression RMSE: {rmse_log:.4f}")
print(f"Random Forest RMSE: {rmse_rf:.4f}")

rmse_df = pd.DataFrame({
    "Model": ["Logistic Regression", "Random Forest"],
    "RMSE": [rmse_log, rmse_rf]
})